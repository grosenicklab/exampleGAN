{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "202297e3",
   "metadata": {},
   "source": [
    "## vanilla GAN by Goodfellow et al., 2014\n",
    "##### directories that belong with this JN = MNIST and runs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde18986",
   "metadata": {},
   "source": [
    "##### https://www.machinecurve.com/index.php/2021/07/17/building-a-simple-vanilla-gan-with-pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8785677",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import uuid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "395a08a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configurable variables \n",
    "\n",
    "num_epochs = 5\n",
    "noise_dimension = 50\n",
    "batch_size = 128\n",
    "train_on_gpu = True\n",
    "unique_run_id = str(uuid.uuid4())\n",
    "stats_after_batch = 50\n",
    "optimizer_lr = 0.0002\n",
    "optimizer_betas = (0.5, 0.999)\n",
    "generator_output_img_shape = 28*28*1 #784\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d426be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Speed ups; makes the code run faster\n",
    "\n",
    "torch.autograd.set_detect_anomaly(False)\n",
    "torch.autograd.profiler.profile(False)\n",
    "torch.autograd.profiler.emit_nvtx(False)\n",
    "torch.backends.cudnn.benchmark = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87d8f720",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generator \n",
    "\n",
    "class Generator(nn.Module):\n",
    "    '''\n",
    "    vanilla GAN generator \n",
    "    '''\n",
    "    def __init__(self,):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "        #1st upsampling\n",
    "        #nn.Linear(noise_dimension, 128, bias=False),\n",
    "        nn.ConvTranspose2d(noise_dimension, 128, kernel_size=2, stride=1, bias=False),\n",
    "        nn.BatchNorm1d(128, 0.8),\n",
    "        nn.LeakyReLU(0.25),\n",
    "        #2nd upsampling\n",
    "        nn.ConvTranspose2d(128, 256, kernel_size=2, stride=1, bias=False),\n",
    "        nn.BatchNorm1d(256, 0.8),\n",
    "        nn.LeakyReLU(0.25),\n",
    "        #3rd upsampling\n",
    "        nn.ConvTranspose2d(256, 512, kernel_size=2, stride=1, bias=False),\n",
    "        nn.BatchNorm1d(512, 0.8),\n",
    "        nn.LeakyReLU(0.25),\n",
    "        #Final upsampling\n",
    "        nn.ConvTranspose2d(512, generator_output_img_shape, kernel_size=2, stride=1, bias=False),\n",
    "        nn.Tanh()\n",
    "    )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''Forward pass'''\n",
    "        return self.layers(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a032a8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CCGAN generator\n",
    "# class cont_cond_cnn_generator(nn.Module):\n",
    "# #    def __init__(self, nz=128, dim_embed=DIM_EMBED, ngf = GEN_SIZE):\n",
    "#     def __init__(self, nz=128, dim_embed=DIM_EMBED, ngf = GEN_SIZE, image_size = IMAGE_SIZE): #new\n",
    "#         super(cont_cond_cnn_generator, self).__init__()\n",
    "#         self.nz = nz\n",
    "#         self.ngf = ngf\n",
    "#         self.dim_embed = dim_embed\n",
    "#         self.image_size = IMAGE_SIZE #new\n",
    "\n",
    "#         self.linear = nn.Linear(nz, 4 * 4 * ngf * 8) #4*4*512\n",
    "\n",
    "#         self.deconv1 = nn.ConvTranspose2d(ngf * 8, ngf * 8, kernel_size=4, stride=2, padding=1)# bias=bias) #h=2h 8\n",
    "#         self.deconv2 = nn.ConvTranspose2d(ngf * 8, ngf * 4, kernel_size=4, stride=2, padding=1)# bias=bias) #h=2h 16\n",
    "#         self.deconv3 = nn.ConvTranspose2d(ngf * 4, ngf * 2, kernel_size=4, stride=2, padding=1)# bias=bias) #h=2h 32\n",
    "#         self.deconv4 = nn.ConvTranspose2d(ngf * 2, ngf, kernel_size=4, stride=2, padding=1)# bias=bias) #h=2h 64\n",
    "#         self.condbn1 = ConditionalBatchNorm2d(ngf * 8, dim_embed)\n",
    "#         self.condbn2 = ConditionalBatchNorm2d(ngf * 4, dim_embed)\n",
    "#         self.condbn3 = ConditionalBatchNorm2d(ngf * 2, dim_embed)\n",
    "#         self.condbn4 = ConditionalBatchNorm2d(ngf, dim_embed)\n",
    "#         self.relu = nn.ReLU()\n",
    "\n",
    "#         self.final_conv = nn.Sequential(\n",
    "#             nn.Conv2d(ngf, ngf, kernel_size=3, stride=1, padding=1, bias=bias), #h=h\n",
    "#             nn.BatchNorm2d(ngf),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv2d(ngf, 1, kernel_size=3, stride=1, padding=1, bias=bias), #h=h\n",
    "#             nn.Tanh()\n",
    "#         )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84ec600e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Discriminator\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    '''\n",
    "    vanilla GAN discriminator\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(generator_output_img_shape, 1024, kernel_size=2, stride=1),\n",
    "            nn.LeakyReLU(0.25),\n",
    "            nn.Conv2d(1024, 512, kernel_size=2, stride=1),\n",
    "            nn.LeakyReLU(0.25),\n",
    "            nn.Conv2d(512, 256, kernel_size=2, stride=1),\n",
    "            nn.LeakyReLU(0.25),\n",
    "            nn.Conv2d(256, 1, kernel_size=2, stride=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''Forward pass'''\n",
    "        return self.layers(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd221816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CCGAN discriminator\n",
    "# class cont_cond_cnn_discriminator(nn.Module):\n",
    "# #    def __init__(self, dim_embed=DIM_EMBED, ndf = DISC_SIZE):\n",
    "#     def __init__(self, dim_embed=DIM_EMBED, ndf = DISC_SIZE, image_size = IMAGE_SIZE): #new\n",
    "#         super(cont_cond_cnn_discriminator, self).__init__()\n",
    "#         self.ndf = ndf\n",
    "#         self.dim_embed = dim_embed\n",
    "#         self.image_size = IMAGE_SIZE #new\n",
    "\n",
    "#         # FLAG commenting out to remove linear layer\n",
    "# #         self.linearB = nn.Linear(image_size*image_size, 64*64, bias=False) #STP 10/8\n",
    "        \n",
    "#         # input is (nc) x 64 x 64\n",
    "#         self.conv1 = nn.Conv2d(1, self.ndf, kernel_size=4, stride=2, padding=1, bias=bias) #h=h/2\n",
    "#         self.conv2 = nn.BatchNorm2d(self.ndf)\n",
    "#         self.conv3 = nn.LeakyReLU(0.2, inplace=True)\n",
    "#         # input is ndf x 32 x 32\n",
    "#         self.conv4 = nn.Conv2d(self.ndf, self.ndf*2, kernel_size=4, stride=2, padding=1, bias=bias) #h=h/2\n",
    "#         self.conv5 = nn.BatchNorm2d(self.ndf*2)\n",
    "#         self.conv6 = nn.LeakyReLU(0.2, inplace=True)\n",
    "#         # input is (ndf*2) x 16 x 16\n",
    "#         self.conv7 = nn.Conv2d(self.ndf*2, self.ndf*4, kernel_size=4, stride=2, padding=1, bias=bias) #h=h/2\n",
    "#         self.conv8 = nn.BatchNorm2d(self.ndf*4)\n",
    "#         self.conv9 = nn.LeakyReLU(0.2, inplace=True)\n",
    "#         # input is (ndf*4) x 8 x 8\n",
    "#         self.conv10 = nn.Conv2d(self.ndf*4, self.ndf*8, kernel_size=4, stride=2, padding=1, bias=bias) #h=h/2\n",
    "#         # nn.BatchNorm2d(self.ndf*8),\n",
    "#         self.conv11 = nn.LeakyReLU(0.2, inplace=True)\n",
    "\n",
    "#         # nn.Conv2d(self.ndf*8, self.ndf*8, 3, stride=1, padding=1, bias=bias),  #h=h                                                                                                       \n",
    "#         # nn.LeakyReLU(0.2, inplace=True) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96657b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Housekeeping functions (device, directory creation, image generation, saving models, printing training progress)\n",
    "\n",
    "def get_device():\n",
    "    '''Gets device based on settings and availability'''\n",
    "    return torch.device(\"cuda:0\" if torch.cuda.is_available() and train_on_gpu else \"cpu\")\n",
    "\n",
    "def make_directory_for_run():\n",
    "    \"\"\" Make a directory for this training run. \"\"\"\n",
    "    print(f'Preparing training run {unique_run_id}')\n",
    "    if not os.path.exists('./runs'):\n",
    "        os.mkdir('./runs')\n",
    "    os.mkdir(f'./runs/{unique_run_id}')\n",
    "\n",
    "def generate_image(generator, epoch = 0, batch = 0, device=get_device()):\n",
    "    \"\"\" Generate subplots with generated examples. \"\"\"\n",
    "    images = []\n",
    "    noise = generate_noise(batch_size, device=device)\n",
    "    generator.eval()\n",
    "    images = generator(noise)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i in range(16):\n",
    "    # Get image\n",
    "        image = images[i]\n",
    "    # Convert image back onto CPU and reshape\n",
    "        image = image.cpu().detach().numpy()\n",
    "        image = np.reshape(image, (28, 28))\n",
    "    # Plot\n",
    "        plt.subplot(4, 4, i+1)\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.axis('off')\n",
    "    if not os.path.exists(f'./runs/{unique_run_id}/images'):\n",
    "        os.mkdir(f'./runs/{unique_run_id}/images')\n",
    "    print(f'./runs/{unique_run_id}/images/epoch{epoch}_batch{batch}.jpg') \n",
    "    plt.savefig(f'./runs/{unique_run_id}/images/epoch{epoch}_batch{batch}.jpg')\n",
    "\n",
    "def save_models(generator, discriminator, epoch):\n",
    "    \"\"\" Save models at specific point in time. \"\"\"\n",
    "    torch.save(generator.state_dict(), f'./runs/{unique_run_id}/generator_{epoch}.pth')\n",
    "    torch.save(discriminator.state_dict(), f'./runs/{unique_run_id}/discriminator_{epoch}.pth')\n",
    "    \n",
    "def print_training_progress(batch, generator_loss, discriminator_loss):\n",
    "    \"\"\" Print training progress. \"\"\"\n",
    "    print('Losses after mini-batch %5d: generator %e, discriminator %e' % (batch, generator_loss, discriminator_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4023aef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Forwards and backwards pass\n",
    "\n",
    "def generate_noise(number_of_images = 1, noise_dimension = noise_dimension, device=None):\n",
    "    \"\"\" Generate noise for number_of_images images, with a specific noise_dimension \"\"\"\n",
    "    return torch.randn(number_of_images, noise_dimension, device=device)\n",
    "\n",
    "\n",
    "def efficient_zero_grad(model):\n",
    "    \"\"\" \n",
    "    Apply zero_grad more efficiently\n",
    "    Source: https://betterprogramming.pub/how-to-make-your-pytorch-code-run-faster-93079f3c1f7b\n",
    "    \"\"\"\n",
    "    for param in model.parameters():\n",
    "        param.grad = None\n",
    "\n",
    "        \n",
    "def forward_and_backward(model, data, loss_function, targets):\n",
    "    \"\"\"\n",
    "    Perform forward and backward pass in a generic way. Returns loss value.\n",
    "    \"\"\"\n",
    "    outputs = model(data)\n",
    "    error = loss_function(outputs, targets)\n",
    "    error.backward()\n",
    "    return error.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cd84d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing dataset\n",
    "\n",
    "def prepare_dataset():\n",
    "    \"\"\" Prepare dataset through DataLoader \"\"\"\n",
    "  # Prepare MNIST dataset\n",
    "    dataset = MNIST(os.getcwd(), download=True, train=True, transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "  ]))\n",
    "    \n",
    "  # Batch and shuffle data with DataLoader\n",
    "    trainloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "  # print(\"here's the trainloader:\", trainloader)\n",
    "  # Return dataset through DataLoader\n",
    "    return trainloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7cd66d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialization functions (models, loss and optimizers)\n",
    "\n",
    "def initialize_models(device = get_device()):\n",
    "    \"\"\" Initialize Generator and Discriminator models \"\"\"\n",
    "    generator = Generator()\n",
    "    discriminator = Discriminator()\n",
    "  # Move models to specific device\n",
    "    generator.to(device)\n",
    "    discriminator.to(device)\n",
    "  # Return models\n",
    "    return generator, discriminator\n",
    "\n",
    "def initialize_loss():\n",
    "    \"\"\" Initialize loss function. \"\"\"\n",
    "    return nn.BCELoss()\n",
    "\n",
    "def initialize_optimizers(generator, discriminator):\n",
    "    \"\"\" Initialize optimizers for Generator and Discriminator. \"\"\"\n",
    "    generator_optimizer = torch.optim.AdamW(generator.parameters(), lr=optimizer_lr,betas=optimizer_betas)\n",
    "    discriminator_optimizer = torch.optim.AdamW(discriminator.parameters(), lr=optimizer_lr,betas=optimizer_betas)\n",
    "    return generator_optimizer, discriminator_optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd720c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training step\n",
    "\n",
    "def perform_train_step(generator, discriminator, real_data, loss_function, generator_optimizer, discriminator_optimizer, device = get_device()):\n",
    "    \"\"\" Perform a single training step. \"\"\"\n",
    "  \n",
    "  ## 1. PREPARATION = Set real and fake labels.\n",
    "    real_label, fake_label = 1.0, 0.0\n",
    "  # Get images on CPU or GPU as configured and available\n",
    "  # Also set 'actual batch size', whih can be smaller than BATCH_SIZE in some cases.\n",
    "    real_images = real_data[0].to(device)\n",
    "    actual_batch_size = real_images.size(0)\n",
    "    label = torch.full((actual_batch_size, 1), real_label, device=device)\n",
    "    \n",
    "  ## 2. TRAINING THE DISCRIMINATOR = Zero the gradients for discriminator\n",
    "    efficient_zero_grad(discriminator)\n",
    "  # Forward + backward on real images, reshaped\n",
    "    real_images = real_images.view(real_images.size(0), -1)\n",
    "    error_real_images = forward_and_backward(discriminator, real_images, loss_function, label)\n",
    "  # Forward + backward on generated images\n",
    "    noise = generate_noise(actual_batch_size, device=device)\n",
    "    generated_images = generator(noise)\n",
    "    label.fill_(fake_label)\n",
    "    error_generated_images =forward_and_backward(discriminator, generated_images.detach(), loss_function, label)\n",
    "  # Optim for discriminator\n",
    "    discriminator_optimizer.step()\n",
    "  \n",
    "  ## 3. TRAINING THE GENERATOR = Forward + backward + optim for generator, including zero grad\n",
    "    efficient_zero_grad(generator)\n",
    "    label.fill_(real_label)\n",
    "    error_generator = forward_and_backward(discriminator, generated_images, loss_function, label)\n",
    "    generator_optimizer.step()\n",
    "  \n",
    "  ## 4. COMPUTING RESULTS = Compute loss values in floats for discriminator, which is joint loss.\n",
    "    error_discriminator = error_real_images + error_generated_images\n",
    "  # Return generator and discriminator loss so that it can be printed.\n",
    "\n",
    "    return error_generator, error_discriminator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2969d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform epoch\n",
    "\n",
    "def perform_epoch(dataloader, generator, discriminator, loss_function, generator_optimizer, discriminator_optimizer, epoch):\n",
    "    \"\"\" Perform a single epoch. \"\"\"\n",
    "    for batch_no, real_data in enumerate(dataloader, 0):\n",
    "        #print(batch_no, real_data)\n",
    "        # Perform training step\n",
    "        generator_loss_val, discriminator_loss_val = perform_train_step(generator, discriminator, real_data, loss_function, generator_optimizer, discriminator_optimizer)\n",
    "        # Print statistics and generate image after every n-th batch\n",
    "        if batch_no % stats_after_batch == 0:\n",
    "            #print(batch_no)\n",
    "            print_training_progress(batch_no, generator_loss_val, discriminator_loss_val)\n",
    "            generate_image(generator, epoch, batch_no)\n",
    "        \n",
    "    # Save models on epoch completion.\n",
    "    save_models(generator, discriminator, epoch)\n",
    "    # Clear memory after every epoch\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6523ca27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing training run a62230b3-41e9-4cd5-a0bb-477c78aac25d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stp4007/.conda/envs/ccganpytorch-new/lib/python3.9/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 4-dimensional input for 4-dimensional weight [1024, 784, 2, 2], but got 2-dimensional input of size [128, 784] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-499151ea01e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mtrain_dcgan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-499151ea01e7>\u001b[0m in \u001b[0;36mtrain_dcgan\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Starting epoch {epoch}...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mperform_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m   \u001b[0;31m# Finished :-)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Finished unique run {unique_run_id}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-a32b45d1fb7e>\u001b[0m in \u001b[0;36mperform_epoch\u001b[0;34m(dataloader, generator, discriminator, loss_function, generator_optimizer, discriminator_optimizer, epoch)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;31m#print(batch_no, real_data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m# Perform training step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mgenerator_loss_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator_loss_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperform_train_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;31m# Print statistics and generate image after every n-th batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_no\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstats_after_batch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-f6a7021a85ee>\u001b[0m in \u001b[0;36mperform_train_step\u001b[0;34m(generator, discriminator, real_data, loss_function, generator_optimizer, discriminator_optimizer, device)\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;31m# Forward + backward on real images, reshaped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mreal_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreal_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0merror_real_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_and_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m   \u001b[0;31m# Forward + backward on generated images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_noise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactual_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-2303ef51f7af>\u001b[0m in \u001b[0;36mforward_and_backward\u001b[0;34m(model, data, loss_function, targets)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mPerform\u001b[0m \u001b[0mforward\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbackward\u001b[0m \u001b[0;32mpass\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mgeneric\u001b[0m \u001b[0mway\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mReturns\u001b[0m \u001b[0mloss\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \"\"\"\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ccganpytorch-new/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-5be181de3660>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;34m'''Forward pass'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ccganpytorch-new/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ccganpytorch-new/lib/python3.9/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ccganpytorch-new/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ccganpytorch-new/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ccganpytorch-new/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    440\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 442\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    443\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight [1024, 784, 2, 2], but got 2-dimensional input of size [128, 784] instead"
     ]
    }
   ],
   "source": [
    "#Training\n",
    "\n",
    "def train_dcgan():\n",
    "    \"\"\" Train the DCGAN. \"\"\"\n",
    "  # Make directory for unique run\n",
    "    make_directory_for_run()\n",
    "  # Set fixed random number seed\n",
    "    torch.manual_seed(42)\n",
    "  # Get prepared dataset\n",
    "    dataloader = prepare_dataset()\n",
    "  # Initialize models\n",
    "    generator, discriminator = initialize_models()\n",
    "  # Initialize loss and optimizers\n",
    "    loss_function = initialize_loss()\n",
    "    generator_optimizer, discriminator_optimizer = initialize_optimizers(generator, discriminator)\n",
    "  # Train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Starting epoch {epoch}...')\n",
    "        perform_epoch(dataloader, generator, discriminator, loss_function, generator_optimizer, discriminator_optimizer, epoch)\n",
    "  # Finished :-)\n",
    "        print(f'Finished unique run {unique_run_id}')\n",
    "\n",
    "    return dataloader, generator, discriminator, loss_function, generator_optimizer, discriminator_optimizer, epoch\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_dcgan()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc55e57d",
   "metadata": {},
   "source": [
    "### Edits to have it fit for stride=1 and conv2d layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49333d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Attempting to reshape the dataset with 4 dimensions since it has only two - [128, 784]\n",
    "\n",
    "    # x = torch.randn(3, 4)\n",
    "    # x = torch.unsqueeze(x, dim=-1)\n",
    "    # x.shape\n",
    "\n",
    "    # # Expected result\n",
    "    # # torch.Size([3, 4, 1])\n",
    "\n",
    "def prepare_dataset():\n",
    "    \"\"\" Prepare dataset through DataLoader \"\"\"\n",
    "  # Prepare MNIST dataset\n",
    "    dataset = MNIST(os.getcwd(), download=True, train=True, transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "  ]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ec6a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Attempting to reshape the dataset with 4 dimensions since it has only two - [128, 784]\n",
    "\n",
    "dataset = prepare_dataset()\n",
    "dataset = torch.reshape(dataset, 784)\n",
    "dataset.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b62853d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e15d282d",
   "metadata": {},
   "source": [
    "### How to load (\"see\") saved models\n",
    "\n",
    "#### https://pytorch.org/tutorials/beginner/saving_loading_models.html"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0de44fe8",
   "metadata": {},
   "source": [
    "model = TheModelClass(*args, **kwargs)\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c410ebbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Discriminator()\n",
    "# model.load_state_dict(torch.load('/home/stp4007/exampleGAN/runs/53f66de5-8c5f-4a42-a2fc-7a2a8d5094d2/discriminator_4.pth'))\n",
    "# model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a749029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Discriminator()\n",
    "# model.load_state_dict(torch.load('/home/stp4007/exampleGAN/runs/53f66de5-8c5f-4a42-a2fc-7a2a8d5094d2/discriminator_4.pth'))\n",
    "# model.eval()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
